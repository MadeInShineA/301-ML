{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f264e933-2407-46b2-9140-78cef5821bed",
   "metadata": {},
   "source": [
    "# Data Pipeline - (Machine) Learning from Disaster\n",
    "\n",
    "In this notebook, you will implement a full machine learning workflow, from preprocessing to model evaluation. \n",
    "The dataset we use is a famous one: [the Titanic dataset](https://www.kaggle.com/competitions/titanic/data) (yes, the big boat that sank...).\n",
    "\n",
    "The idea is simple: use ML to predict which passengers **survived** the Titanic shipwreck. The dataset is quite simple to understand but presents some real-world challenges (e.g., missing values). The explanation of the dataset is available at the link above.\n",
    "\n",
    "## Goals\n",
    "The *first* and most important goal of this lab is to guide you towards a higher level of autonomy when dealing with ML problems, in particular, classification problems (and in the later part, optionally, you will deal with a regression problem). \n",
    "\n",
    "The *second* goal is to get an understanding of the Support Vector Machine (SVM) algorithm, which is a (relatively) simple but powerful ML algorithm.\n",
    "\n",
    "This document provides just the skeleton of your program, reminding you of the main steps to be accomplished.\n",
    "At the end of this lab, you will be able to:\n",
    "- Work on a jupyter notebook for a ML problem.\n",
    "- Develop a full Machine Learning pipeline starting from a skeleton.\n",
    "- Perform data exploration and data preparation\n",
    "- Train, tune and **properly** evaluate ML models.\n",
    "\n",
    "## Structure\n",
    "This notebook is divided into 3 main parts:\n",
    "1. **Data Exploration**: where you will explore the dataset using the Pandas library.\n",
    "2. **Data Preparation**: where you will preprocess the data to be used in the ML model.\n",
    "3. **Modeling, fine-tuning and evaluation**: where you will train, tune and evaluate the ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28f15c0e187712",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1 Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4c430963c87ac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.1 Read the data\n",
    "We dowloaded the dataset for you. You will find the .csv file in the `data` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1626b7ef17edde5a",
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'c:/Users/ingno/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# some useful imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# read csv from data folder\n",
    "df = pd.read_csv('data/dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bc2ac0e9b11b4",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the cell above, we read and stored the data in a pandas *dataframe*.\n",
    "A dataframe is a 2-dimensional labeled data structure with columns of potentially different types.\n",
    "You can think of it like a spreadsheet or SQL table, or a dictionary of Series objects.\n",
    "\n",
    "In the next cells, we will start exploring the data while starting to learn how to use the most important functions of the pandas library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1656c59fcb2d77db",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.2 Have a look to the data\n",
    "\n",
    "In Machine Learning, the **first** thing we want to do is to look at the data. *Always*.\n",
    "This page https://www.kaggle.com/competitions/titanic/data contains the so-called \"metadata\".\n",
    "Metadata refers to data that provides information about other data. It describes various aspects of data, such as its origin, format, structure, and context, making it easier to understand, manage, and use. Metadata can be crucial for organizing, locating, and interpreting different types of information within a dataset or system.\n",
    "\n",
    "In the metadata description you can find information about the data we are going to use. Give it a look."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4660635431f0a59c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question**:\n",
    "Checking the data card of the model, how many features are there? How many labels? (or target?). In other terms, if you would like to put this data in a tabel-like structure, how many rows would you have? How many columns?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61587c5f74307eac",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22297c2427cd4a0d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's see if the data are consistent with the metadata.\n",
    "To do that, we will use our first pandas function: `head()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fe8d9ecaee51b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.309463600Z",
     "start_time": "2023-11-14T18:12:23.293510800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we print the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce599195cc5c6ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.346333900Z",
     "start_time": "2023-11-14T18:12:23.308461Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we print the first 10 rows of the dataframe\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350b91a75064eb7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.347330800Z",
     "start_time": "2023-11-14T18:12:23.317431100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we also print the last 5 rows of the dataframe\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdc89046c686394",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "To get a better understanding of the data, we can also print the shape of the dataframe (i.e., number of rows, columns, etc.), the column names and data types.\n",
    "Finally, we can also print the descriptive statistics of the data.\n",
    "This is done using the functions `shape`, `info` and `describe`, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4392760bd533173d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.370255600Z",
     "start_time": "2023-11-14T18:12:23.327397200Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we print the shape of the dataframe\n",
    "df.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cc71fe58ad96b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.370255600Z",
     "start_time": "2023-11-14T18:12:23.332380300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we print the column names and data types\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4888331be24d48",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.448991200Z",
     "start_time": "2023-11-14T18:12:23.345337100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we print the descriptive statistics of the dataframe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6d424cb16b83b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Questions**:\n",
    "1. Is the data consistent with the metadata?\n",
    "2. Which columns are features? Which columns are labels? (or target?)\n",
    "3. Among the features, there is one that is not correlated with any possible useful prediction. Which one and why?\n",
    "4. What is the mean of the target?\n",
    "5. Why not all columns are present in the descriptive statistics?\n",
    "6. Is the dataset in your opinion \"balanced\" with respect to the column `Survived` (hint: try the method `df['Survived'].value_counts()`) ? If not, can this be a problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1abe80b07aeaeb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer**:\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7690ee5b78d755e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.491846400Z",
     "start_time": "2023-11-14T18:12:23.361283900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we print the value counts of the column Survived to check for imbalance\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4f36f",
   "metadata": {},
   "source": [
    "### 1.3 Missing data\n",
    "In this subsection we check if our dataset is complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07474f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print the number of missing values in the dataframe\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7ce8ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the percentage of missing values in the dataframe\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90041d",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- Are there missing values in the dataset?\n",
    "- If yes, how many missing values are there in each column?\n",
    "- Are there missing values in the target column? If yes, how many?\n",
    "\n",
    "NOTES: Knowing the purcentage of missing values and their type (i.e., numerical, categorical, etc.) is crucial for deciding how to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad75933",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce34a6f",
   "metadata": {},
   "source": [
    "### 1.4 Correlation analysis\n",
    "In this subsection, we check the correlation among features and between features and the target. We will plot a correlation matrix and a heatmap.\n",
    "\n",
    "NOTE: For simplicity, we will only consider numerical features. We could also consider categorical features by converting them to numerical ones (e.g., using one-hot encoding, see next section)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeba67e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of numerical variables\n",
    "numerical_features = ['PassengerId', 'Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\n",
    "\n",
    "# list of categorical variables\n",
    "categorical_features = ['Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae48dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select the numerical columns \n",
    "\n",
    "df_numerical = df[numerical_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c12f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot the correlation matrix of the numerical variables (it can take some time, depending on the size of the data)\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "correlation_matrix = df_numerical.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True)\n",
    "plt.title('Correlation Matrix of Numerical Variables')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fee782",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "1. What is the most correlated feature with the target `Survived`? What does it means?\n",
    "2. What is the less correlated feature with the target `Survived`? Is that a surprise?\n",
    "3. What is the most correlated feature with the feature `Pclass`?\n",
    "4. Is it good to have highly correlated features in the dataset? Why?\n",
    "5. What is the meaning of a negative correlation?\n",
    "6. What is the meaning of high correlation between a feature and the target?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70796939",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "#TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8396bb9eb8f75dfe",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Extra - Getting used to pandas\n",
    "\n",
    "This is not part of the usal ML pipeline, but it is important to get used to pandas. In the previous cells, we learned how to read data from a csv file and how to get a first look at the data.\n",
    "\n",
    "In this subsection, we will practice some pandas commands that will help us in the next sections and labs.\n",
    "In particular, we will learn how to select columns and rows from a dataframe and how to filter data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923c1483a31e0853",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### E.1 Selecting columns using columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d2dfa2dd7b4cc7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.536697Z",
     "start_time": "2023-11-14T18:12:23.369258800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the column 'Ticket' and store it in a variable called ticket;\n",
    "# ticket is a pandas series (you can think of it as a 1-dimensional array)\n",
    "ticket = df['Ticket']\n",
    "\n",
    "# we print the first 5 rows of the air_temperature variable\n",
    "ticket.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b28df15af290763",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "In the previous example, we selected a column from the dataframe.\n",
    "We can now try to select multiple columns from the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb2ea53f117051d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.537694100Z",
     "start_time": "2023-11-14T18:12:23.374242400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the columns 'Ticket' and 'Fare' and store them in a variable called ticket_and_fare;\n",
    "# ticket_and_fare is a pandas dataframe (you can think of it as a 2-dimensional array)\n",
    "ticket_and_fare = df[['Ticket', 'Fare']]\n",
    "\n",
    "# we print the first 5 rows of the temperatures variable\n",
    "ticket_and_fare.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd8ecd62b074e9f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### E.2 Selecting rows and columns using their index (iloc)\n",
    "We can also select rows from the dataframe.\n",
    "`iloc` is a method that allows us to select rows from the dataframe. It returns a row as a pandas series or a dataframe (if more than one raw are selected)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c946bdeb208fc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.537694100Z",
     "start_time": "2023-11-14T18:12:23.380222300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the first row of the dataframe\n",
    "df.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd0033d-3cb1-4249-8858-f35cea5ca93a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.537694100Z",
     "start_time": "2023-11-14T18:12:23.386202200Z"
    }
   },
   "outputs": [],
   "source": [
    "# we select multiple rows from the dataframe\n",
    "df.iloc[[0, 1, 2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14dbfb4df17906c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Like lists, numpy arrays, etc. it is possible to \"slice\" pandas series and dataframes. We use the method `.iloc()` Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf4d306f5e610c1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.537694100Z",
     "start_time": "2023-11-14T18:12:23.397165900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the first 5 rows of the dataframe\n",
    "df.iloc[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7e59b825e80d86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.538690300Z",
     "start_time": "2023-11-14T18:12:23.407133400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the last 5 rows of the dataframe\n",
    "df.iloc[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f954c548443dcea6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.538690300Z",
     "start_time": "2023-11-14T18:12:23.416100400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the rows from index 5 to index 10\n",
    "df.iloc[5:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa5f60a3869e3d1",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "With `iloc`, we can also select columns from the dataframe. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b7a0bbd7c733af",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.556629800Z",
     "start_time": "2023-11-14T18:12:23.425070500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the columns from index 0 to index 2\n",
    "df.iloc[:, 0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef39881d93d949f3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.557626900Z",
     "start_time": "2023-11-14T18:12:23.433043600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the rows from index 5 to index 10 and the columns from index 0 to index 2\n",
    "df.iloc[5:10, 0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99123c1324cdc8",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Question**:\n",
    "1. Select the first 5 rows of the dataframe and the columns from index 0 to index 2.\n",
    "2. Select the last 5 rows of the dataframe and the columns from index 0 to index 2.\n",
    "3. Select the rows from index 15 to index 25 and the columns from index 2 to index 3.\n",
    "\n",
    "*Suggestion: do not hesitate to add multiple cells to answer the questions.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e020a6519ed46e8c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answers**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdfae9327a81fd2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.557626900Z",
     "start_time": "2023-11-14T18:12:23.440020600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Select the first 5 rows of the dataframe and the columns from index 0 to index 2.\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778485ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Select the last 5 rows of the dataframe and the columns from index 1 to index 3.\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select the first 5 rows of the dataframe and the 'Ticket' and 'Fare' columns.\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10edce576835dd6b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### E.3 Selecting rows and columns using their labels (a.k.a. name) (loc)\n",
    "\n",
    "`iloc()` is not the only method that allows us to select rows from a dataframe.\n",
    "We can also use `loc()`. The difference between `iloc()` and `loc()` is that `iloc()` selects rows by index and `loc()` selects rows by label. Here are some examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a32e53beb57a372",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.660282700Z",
     "start_time": "2023-11-14T18:12:23.451981500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the first row of the dataframe\n",
    "df.loc[0]\n",
    "\n",
    "# we select multiple rows from the dataframe\n",
    "df.loc[[0, 1, 2]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b850f57a1f5a22",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "As you can see there is not much of a difference when working with *rows*, since rows are *usually* indexed with numbers.\n",
    "However, when working with **columns**, `loc()` is arguably more useful (and readable) than `iloc()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7180bf71415c9a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.661279600Z",
     "start_time": "2023-11-14T18:12:23.461946500Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the column 'SibSp' using loc\n",
    "df.loc[:, 'SibSp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf98909867c43bcc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.678225200Z",
     "start_time": "2023-11-14T18:12:23.467926800Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the columns 'SibSp' and 'Sex' using loc\n",
    "df.loc[:, ['SibSp', 'Sex']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42be6676672bbe5",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Questions**:\n",
    "1. Select the first 5 rows of the dataframe and the column `Ticket` using loc.\n",
    "2. Select 10 rows starting at the index 11 for the columns`Fare` and `Sex` using loc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60a7397c71c32d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad838f09338b87c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.678225200Z",
     "start_time": "2023-11-14T18:12:23.476896400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Select the first 5 rows of the dataframe and the column `Ticket` using loc.\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b53c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Select 10 rows starting at the index 11 for the columns`Fare` and `Sex` using loc.\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e3764b",
   "metadata": {},
   "source": [
    "**NOTE 1**:  test1 = df.loc[:, 'Fare'] is equivalent to test2 = df['Fare']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02cc3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Is **test1** = df.loc[:, 'Fare'] equivalent to **test2** = df['Fare']? \n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74107397-c2ed-4236-9f27-11d3d704b56a",
   "metadata": {},
   "source": [
    "**NOTE 2**: by default we are **NOT** creating new dataframes. Dataframes maybe very big, copying them will take too much resources. In the previous exemple, `test1` and `test2` are pointing at the *same* address in the memory. In other terms, by default, we work with \"views\" on the same dataset.\n",
    "You can check this with the `is` operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb854446-196a-482a-8aec-6de95b80ec1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.680216300Z",
     "start_time": "2023-11-14T18:12:23.500816600Z"
    }
   },
   "outputs": [],
   "source": [
    "if test1 is test2:\n",
    "    print(\"yup, told ya, the two variables point to the same object in memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677140ba-54fd-4d15-90e9-c8448ce8dcf3",
   "metadata": {},
   "source": [
    "**For the sake of completeness**: you may need to copy a dataframe. If you want to duplicate a dataframe, you have to use the method `.copy()`, see the [official documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html) for more information. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf60c6be4957f",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### E.4 Filtering data\n",
    "Last but not least, with pandas we can also easily filter data. Filtering data means selecting rows that satisfy a certain condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a628d7a96496e0c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.831709200Z",
     "start_time": "2023-11-14T18:12:23.505800600Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the rows where the column 'Sex' is equal to 'male' (i.e., we only select male passengers)\n",
    "df[df['Sex'] == 'male']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37d7e13ea8a9bc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.831709200Z",
     "start_time": "2023-11-14T18:12:23.517760400Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we select the rows where the column 'Pclass' is equal to 2 (i.e, we select the passenger travelling in second class)\n",
    "df[df['Pclass'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4e23d317567dd2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Questions**:\n",
    "1. Select the passengers travelling in 3rd class.\n",
    "2. Select the passengers older than 18\n",
    "3. (hard) Select the *famale* passengers with strinctly more than 1 parent / children aboard the Titanic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc1ebf02626cec",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "**Answer**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734986cd05f81fa6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.831709200Z",
     "start_time": "2023-11-14T18:12:23.529719900Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 1. Select the passengers travelling in 3rd class.\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc1b6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Select the passengers older than 18 (18 included).\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49093db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Select the famale passengers with strinctly more than 1 parent / children aboard the Titanic\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aae86fa22a28f8e",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.3 Conclusion Section 1\n",
    "In this Section, we learned how to read data from a csv file and how to get a look at the data using the Pandas library. \n",
    "We also learned how to select columns and rows from a dataframe using the Pandas library and how to filter data using the Pandas library.\n",
    "\n",
    "Today, Pandas is almost ubiquitous in machine learning when coding in Python. If the methods we’ve covered so far aren’t yet clear to you, don’t worry—it will become easier as you practice these techniques.\n",
    "\n",
    "In the next secgtion, we will learn how to prepare the data for the training using another famous library: **Scikit-learn**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490096835d388b17",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-14T18:12:23.927389200Z",
     "start_time": "2023-11-14T18:12:23.570583900Z"
    },
    "collapsed": false
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd6502e",
   "metadata": {},
   "source": [
    "## 2. Data preparation and introduction to Scikit-learn\n",
    "\n",
    "In this second notebook, our primary emphasis will be on data preparation. This involves the essential steps of dividing the dataset into a *training set* and a *test set*, along with gaining insights into best practices for addressing specific issues or characteristics within our data. These characteristics may include:\n",
    "\n",
    "- Features with varying scales\n",
    "- The intermixing of numerical features (e.g., `Age`) and categorical features (e.g., `Sex`)\n",
    "- Handling missing values\n",
    "\n",
    "To tackle these challenges, we will use [Scikit-learn](https://scikit-learn.org/stable/). Scikit-learn, also known as sklearn, is an open-source machine learning library for the Python programming language. It is built on NumPy, SciPy, and Matplotlib, and it provides simple and efficient tools for data analysis and modeling. Scikit-learn is designed to work with a variety of machine learning tasks, including classification, regression, clustering, dimensionality reduction, and more.\n",
    "\n",
    "Scikit-learn is widely used in both academia and industry for its user-friendly interface, extensive documentation, and the wealth of tools it offers for building and evaluating machine learning models.\n",
    "\n",
    "\n",
    "**Goals**\n",
    "\n",
    "In this second Section, we will focus on data preparation. In particular, you will learn how to use the Scikit-learn library to:\n",
    "- Split the data into training and test sets\n",
    "- Impute missing values\n",
    "- Scale and/or standardize the data\n",
    "- Encode categorical features\n",
    "\n",
    "Remember: all the insights used for data preparation (e.g., values and logic used for imputation, scaling, etc.) should be learned from the training set **only**. Then, the same approach should be applied to training and test set. This is crucial to avoid data leakage and to ensure that the model is trained and evaluated on the same data distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14286501",
   "metadata": {},
   "source": [
    "### 2.1 Separate the labels from the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5619f0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print (again) the first rows of the dataframe (for recap and better visualization)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fd8381",
   "metadata": {},
   "source": [
    "Checking the columns, and with the information we got drom the Data Exploration section, we separate the columns into multiple lists:\n",
    "- useless_columns: columns that are not useful for the model (e.g., `PassengerId`)\n",
    "- numerical_columns: columns that are numerical (e.g., `Age`, `Fare`)\n",
    "- categorical_columns: columns that are categorical (e.g., `Sex`, `Embarked`)\n",
    "- target_column: the column we want to predict (e.g., `Survived`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986d6653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below, we also remove the \"name\" column. However, with an appropriate feature engineering,\n",
    "# we could extract useful information from the name column such as the title\n",
    "# (not only Mr., Mrs., Miss, but also Dr., Col., etc. which may or may not be correlated to an higher suvival rate)\n",
    "usless_columns = #TODO \n",
    "\n",
    "# Target variable(s), in this case, only one\n",
    "target = #TODO\n",
    "\n",
    "# list of numerical variables\n",
    "numerical_features = #TODO\n",
    "\n",
    "# list of categorical variables\n",
    "categorical_features = #TODO\n",
    "\n",
    "\n",
    "#NOTE: be sure that a column is not in more than one list and that all columns are in one of the lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476b1714",
   "metadata": {},
   "source": [
    "Before splitting the data into *training* and *test sets*, we need to separate the *labels* from the *features*. It is of **paramount importance** not providing the labels to the model during training!\n",
    "\n",
    "This is the *noobest* mistake you can do in ML. It is like providing a test with the solutions: the model will learn the right solution \"by heart\" without actually understanding the data. Then, when you will provide new data, with no solution... good luck with that!\n",
    "\n",
    "NOTES:\n",
    "- In ML, the letter `X` usually indicates the features and `y` the labels. Here, we will use `X` and `y` to store the features and labels, respectively. We can do this first separation by using the `drop()` method of the Pandas library.\n",
    "- the `drop()` method returns a **copy** of the dataframe with the specified columns removed. The `drop()` method does not change the original dataframe.\n",
    "- since we are dropping columns, we will also drop usless columns (e.g., `PassengerId`). We will also drop the column `Cabin` because it has too many missing values (which was a precious insight from the Data Exploration section).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d581d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we select the target to create our target variable y\n",
    "y = df[target]\n",
    "\n",
    "# we drop the target from the dataframe to create our features X\n",
    "X = #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc0826a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print the first 5 rows of the features\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fdb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print the first 5 rows of the labels\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f546c8",
   "metadata": {},
   "source": [
    "From the insights that we got in the previous section, we drop the features that we think are not useful for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c18dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we drop the columns in the useless_columns list from the features\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37918fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print the first 10 rows of the features\n",
    "X.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cab2a98",
   "metadata": {},
   "source": [
    "NOTEs:\n",
    "- there is a missing value in the raw 5 for the column `Age`.\n",
    "- now that we splitted the data into features and labels, we need to be careful in the next steps. Some methods require shuffling the data: if you change the order of the features, you will have to change the order of the labels accordingly!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0bfd6b",
   "metadata": {},
   "source": [
    "### 2.2 Split the data into training and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc0a41",
   "metadata": {},
   "source": [
    "It is now time to actually split the data into \"Training\" and \"Test\" set. Remember: the test set will only be used for testing our model. NO learning should use these data. Once splitted, it is like these data do not exist for you until the final evaluation step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f9b925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the sklearn library we import the function to split the data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# we split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # random_state is fixed for reproducibility. Do not change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68b8890",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the shape of the original dataset\n",
    "print('The shape of the original dataset is:', df.shape)\n",
    "\n",
    "# we check the shape of the training set (features)\n",
    "print('The shape of the training set is:', X_train.shape)\n",
    "\n",
    "# we check the shape of the test set (features)\n",
    "print('The shape of the test set is:', X_test.shape)\n",
    "\n",
    "# we check the shape of the training set (labels)\n",
    "print('The shape of the training labels is:', y_train.shape)\n",
    "\n",
    "# we check the shape of the test set (labels)\n",
    "print('The shape of the test labels is:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f4c97",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- What is the purpose of the `test_size` parameter?\n",
    "- What is the purpose of the `random_state` parameter? Should I change this parameter to find the one that gives the best results?\n",
    "- Do we need to shuffle the data? Why is it important to shuffle (or not) the data before splitting it? (Hint: check in the doc the default value of the `shuffle` parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2faf8172",
   "metadata": {},
   "source": [
    "**Answers**: \n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f439f0",
   "metadata": {},
   "source": [
    "### 2.3 Impute missing values\n",
    "\n",
    "In this section, we will learn how to impute missing values using the Scikit-learn library.\n",
    "\n",
    "Imputation\n",
    "- Definition: the process of replacing missing data with substituted values.\n",
    "- Example: we have missing values about the \"age\" of some passengers. Instead of dropping the whole raw of data, we substitute the missing age value with the average value of the age in the trining set.\n",
    "- Be aware:\n",
    "    - This is a trade-off: we introduce a bias in the data instead of losing some information.\n",
    "    - Depending on some factors such as the size of the original dataset, the amount of missing data, this can be a good or a bad idea. If you have a big dataset, losing some data is not a big deal. If you have a small dataset, losing some data can be a big deal.\n",
    "    - Never impute missing values in the target (y). You should drop the rows with missing values in the target.\n",
    "\n",
    "Missing values\n",
    "\n",
    "Missing values are often represented as `NaN` (Not a Number) or `None`. In the code below, we use the `isnull()` method of the Pandas library to check if there are any missing values in the dataset. The `isnull()` method returns a dataframe of booleans that indicate whether or not a value is missing (`True`) or not missing (`False`).\n",
    "Then, we use the `any()` method of the Pandas library to check if the dataframe contains any `True` values. If the dataframe contains any `True` values, then we know that there are missing values in the dataset.\n",
    "\n",
    "**Note (again)**: remember to perform all these tests on the training set ***only***. Always consider as the test set **is not** available during the training phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa30b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for recap, we print again the missing values in the training set\n",
    "# # we check if there are any missing values in the training set\n",
    "missing_values_training_X = X_train.isnull()\n",
    "\n",
    "# we check if test contains any True values\n",
    "print(missing_values_training_X.any())\n",
    "\n",
    "# alternatively, if you want to see the number of missing values per feature\n",
    "# we check the percentage of missing values in the dataframe\n",
    "print()\n",
    "print(missing_values_training_X.mean() * 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee9678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for recap, we print again the missing values in the training set\n",
    "# # we check if there are any missing values in the training set\n",
    "missing_values_training_y = y_train.isnull()\n",
    "\n",
    "# we check if test contains any True values\n",
    "print(missing_values_training_y.any())\n",
    "\n",
    "# alternatively, if you want to see the number of missing values per feature\n",
    "# we check the percentage of missing values in the dataframe\n",
    "print()\n",
    "print(missing_values_training_y.mean() * 100 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d6e358",
   "metadata": {},
   "source": [
    "As we already observed, there are missing values in the columns `Age`, and `Embarked` (we removed `Cabin`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db0eef",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "- Repeat the test above to check if there are any missing values in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0697f233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we repeat the test for the test set. In this case, we can expect the same missing values in the test set as in the training set.\n",
    "# however, it is good practice to check it anyway (i.e., in some cases the test set may contain missing values that are not present in the training set)\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22270343",
   "metadata": {},
   "source": [
    "Let us start with the imputation\n",
    "\n",
    "Some algorithms are able to handle missing values without preprocessing. You can find a list of these algorithms [here](https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values). However, most algorithms are not designed to handle missing values. Therefore, we need to impute them before training the model. There are many ways to impute missing values.\n",
    "\n",
    "Possible approaches:\n",
    "1. Drop rows with missing values. If you have a lot of data, this could you best option.\n",
    "2. Impute missing values with the \"mean\" or \"median\". This apporach is suitable for numerical features.\n",
    "3. Impute missing values with the \"mode\" (i.e., the most frequent value).  This apporach is suitable for categorical features (computing the mean does not make sense).\n",
    "4. Impute missing values using more advanced approaches (such as k-nn!)\n",
    "\n",
    "As an exemple, we will see how to implement 2 and 3 using the Scikit-learn library. \n",
    "\n",
    "In our dataset, we have both numerical features (e.g., `Age`) and categorical features (e.g., `Embarked`) with missing values. We will need to deal with them separately.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c0ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we deal with the missing values in the numerical features\n",
    "\n",
    "# we import the SimpleImputer class from the sklearn library\n",
    "# we will use 'mean' as strategy to impute the missing values\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# we create an object of the SimpleImputer class\n",
    "# we start by dealing with the numerical features\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "\n",
    "# we fit the imputer to the training set\n",
    "imputer.fit(X_train[numerical_features])\n",
    "\n",
    "# we transform the training set\n",
    "train_set_imputed_num = imputer.transform(X_train[numerical_features])\n",
    "\n",
    "# we transform the test set\n",
    "# we use the same imputer that was fit on the training set\n",
    "test_set_imputed_num = imputer.transform(X_test[numerical_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7f913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we deal with the missing values in the categorical features\n",
    "\n",
    "# we import the SimpleImputer class from the sklearn library\n",
    "# we will use 'most_frequent' as strategy to impute the missing values\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35f5934",
   "metadata": {},
   "source": [
    "We test again the data to see if there are still some missing values.\n",
    "\n",
    "NOTE: the Scikit-learn `.transform()` method returns a numpy.ndarray not a data frame. To continue working with pandas we need to convert it back to a data frame. However, for the moment, we will keep it as a numpy array since also the next steps require numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117d5a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print the first 10 rows of the training set (there should no be more missing values in the sixth row)\n",
    "train_set_imputed_num[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440a071d",
   "metadata": {},
   "source": [
    "#### Optional task\n",
    "Scikit-lern offers more advanced imputers. For example, the `IterativeImputer` class models each feature with missing values as a function of other features, and uses that estimate for imputation.\n",
    "The `KNNImputer` class imputes missing values using the k-Nearest Neighbors approach (yes, we can use ML to solve ML problems).\n",
    "For more information on these and other classes, see the [Scikit-learn documentation](https://scikit-learn.org/stable/modules/impute.html#impute).\n",
    "\n",
    "Try the `IterativeImputer` class and the `KNNImputer` class to impute the missing values in the training set. Then, check if there are any missing values in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944d09f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO (optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ed5972",
   "metadata": {},
   "source": [
    "### 2.4 Scale and/or standardize the data\n",
    "Rescaling or standardizing the data is useful for many algorithms that suffers when features' scale changes (e.g., KNN, SVM, Neural Networks, etc.). Basically, this concerns all algorithms based on the distance between data points.\n",
    "Other algorithms, such as Decision Trees, are not affected by the scale of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0611eee0",
   "metadata": {},
   "source": [
    "In the code below, we use the `StandardScaler` class to scale and/or standardize the data. The `StandardScaler` class standardizes features by removing the mean and scaling to unit variance. The standard score of a sample `x` is calculated as: \n",
    "\n",
    "$$\n",
    "z = (x - \\mu) / \\sigma\n",
    "$$\n",
    "\n",
    "where $\\mu$ is the mean of the training samples and $\\sigma$ is the standard deviation of the training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the sklearn library we import the function to scale and/or standardize the data\n",
    "from sklearn.preprocessing import StandardScaler # other options include MinMaxScaler, RobustScaler, etc.\n",
    "\n",
    "# we create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# we fit the scaler to the training set\n",
    "#TODO\n",
    "    \n",
    "# we transform the training set\n",
    "#TODO\n",
    "\n",
    "# we transform the test set\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7758d9d",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- We only rescaled the numerical features. Why did we not rescale the categorical features?\n",
    "- Compare the standardized values with the original values. Compute the mean (`np.average()`) and the standard deviation (`np.std()`) of the original and standardized values. What do you expect and what do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd0429",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "- We did not rescale the categorical features because they are not numerical. Rescaling categorical features does not make sense. For example, if we have a categorical feature with values `['A', 'B', 'C']`, rescaling these values would not make sense. The values are not ordered, and the distance between them is not meaningful. However, Categorical features should be encoded before being used in the model.\n",
    "- You should see that the standardized values have a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90262539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we print the average of the scaled training set\n",
    "#TODO\n",
    "\n",
    "# we print the standard deviation of the scaled training set\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9dcee7",
   "metadata": {},
   "source": [
    "Scikit-learn provides a number of other classes for scaling and/or standardizing the data. For example, the `MinMaxScaler` class scales features to a given range, usually between 0 and 1. The `RobustScaler` class scales features using statistics that are robust to outliers. For more information on these and other classes, see the [Scikit-learn documentation](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2a5e1a",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- instead of using the `StandardScaler` class, use the `RobustScaler` class to scale and/or standardize the data. Then, print the first 5 rows of the scaled training set.\n",
    "- What is the difference between the `StandardScaler` and the `RobustScaler`? When should you use one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f654f6f1",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45912ca8",
   "metadata": {},
   "source": [
    "\n",
    "### 2.5 Encode categorical features\n",
    "In the previous section, we saw how to scale and/or standardize the numerical features. In this section, we will learn how to encode the categorical features.\n",
    "\n",
    "Some algorithms do not have any problem to deal with a mixed feature set (i.e., numerical and categorical features). However, other algorithms require that all the features are numerical. Therefore, we need to encode the categorical features before training the model.\n",
    "Typically, distance-based algorithms (e.g., KNN, SVM, Neural Networks, etc.) require that all the features are numerical.\n",
    "How could you compute the distance between a numerical feature and a categorical feature? It does not make sense, right? Therefore, we need to encode the categorical features before training the model.\n",
    "\n",
    "There are many ways to encode categorical features. We use the `OneHotEncoder` class to encode the categorical features. The `OneHotEncoder` class encodes categorical features as a one-hot numeric array. A one-hot array is a binary matrix where each row has exactly one element set to 1 and all other elements set to 0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a842c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before encoding the categorical features, we convert the numpy arrays to dataframes. This is not necessary, but it makes the code more readable (it will help the encoder to generate the correct column names)\n",
    "X_train_set_imputed_cat = pd.DataFrame(train_set_imputed_cat, columns=categorical_features)\n",
    "X_test_set_imputed_cat = pd.DataFrame(test_set_imputed_cat, columns=categorical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e22d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the OneHotEncoder class from the sklearn library\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# we create an OrdinalEncoder object\n",
    "encoder = OneHotEncoder(sparse_output=False) # we set sparse_output=False to get a 2D array\n",
    "\n",
    "# we fit the encoder to the training set\n",
    "#TODO\n",
    "\n",
    "# we transform the training set\n",
    "#TODO\n",
    "\n",
    "# we transform the test set\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e1662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put the encoded data back into a dataframe. The encoder generates understandably named columns for us\n",
    "X_train_set_encoded_df = pd.DataFrame(X_train_set_encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
    "X_test_set_encoded_df = pd.DataFrame(X_test_set_encoded, columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "# we print the first 5 rows of the encoded training set\n",
    "X_train_set_encoded_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bfe436",
   "metadata": {},
   "source": [
    "**Question**\n",
    "- Other encoders exist such as the `OrdinalEncoder`. Explain the difference between the `OrdinalEncoder` and the `OneHotEncoder`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cbe190",
   "metadata": {},
   "source": [
    "**Answer**:\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb98638",
   "metadata": {},
   "source": [
    "### 2.6. Encode the target (label)\n",
    "\n",
    "Finally, in some cases, we need to encode the target (label) as well. Suppose you have to classify \"apple\", \"banana\", and \"orange\". You can encode them as 0, 1, and 2, respectively. This is useful when you have a classification problem with more than two classes. However, in our case, we have a binary classification problem (i.e., survived or not survived). Therefore, we do not need to encode the target. To get more information about encoding the target, see the [Scikit-learn documentation](https://scikit-learn.org/stable/modules/preprocessing_targets.html#preprocessing-targets)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc7ca0d",
   "metadata": {},
   "source": [
    "### 2.7. Putting it all together\n",
    "\n",
    "As a result of steps 5 and 6 we create sub-datasets of the original dataset. In particular, we have:\n",
    "- `X_train_set_scaled`: the training set with scaled and/or standardized numerical features\n",
    "- `X_test_set_scaled`: the test set with scaled and/or standardized numerical features\n",
    "- `X_train_set_encoded`: the training set with encoded categorical features\n",
    "- `X_test_set_encoded`: the test set with encoded categorical features\n",
    "- `y_train_set_encoded`: the training labels with encoded categorical features\n",
    "- `y_test_set_encoded`: the test labels with encoded categorical features\n",
    " \n",
    "Before starting the training and the evaluation in the next section, we need to concatenate the scaled and/or standardized numerical features with the encoded categorical features (we want a pandas dataframe).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552f7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We put back the numerical data in a pandas dataframe (we already did it for the categorical data)\n",
    "X_train_set_scaled = pd.DataFrame(X_train_set_scaled, columns=numerical_features)\n",
    "X_test_set_scaled = pd.DataFrame(X_test_set_scaled, columns=numerical_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abef044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we concatenate the numerical and categorical dataframes to get the final training and test sets\n",
    "X_train_final = pd.concat([X_train_set_scaled, X_train_set_encoded_df], axis=1)\n",
    "X_test_final = pd.concat([X_test_set_scaled, X_test_set_encoded_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2481e0d",
   "metadata": {},
   "source": [
    "Let us do a sanity check to see if everything is ok.\n",
    "\n",
    "We should have the same rows that we had in the original training. The columns number may have changed due to the encoding and dropping of the \"usless_columns\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd4b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we check the shape of the final training and test sets\n",
    "print('The shape of the final training set is:', X_train_final.shape, 'and the shape of y training is:', y_train.shape)\n",
    "print('The shape of the final test set is:', X_test_final.shape, 'and the shape of y test is:', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca49c91",
   "metadata": {},
   "source": [
    "### Conclusion Section 2\n",
    "In this section, we learned how to use the Scikit-learn library to prepare the data for a classification task. In particular, we learned how to:\n",
    "- Split the data into training and test sets\n",
    "- Impute missing values\n",
    "- Scale and/or standardize the data\n",
    "- Encode categorical features\n",
    "- Encode the target (label)\n",
    "\n",
    "In the next section, we will use the Scikit-learn library to train and evaluate a machine learning model to predict if a passenger survived the Titanic disaster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b454b54e",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baeff91e",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Training\n",
    "\n",
    "In this third (and last) section of this lab, we training and we evaluate our model.\n",
    "We will use the Scikit-learn library to train a k-nn model and evaluate its performance. In addition, we will use the Scikit-learn library to find the best hyperparameters for the model and we will print a learning curve to see if the model is overfitting or underfitting.\n",
    "Finally, we will compute performance metrics such as the confusion matrix, the accuracy, the precision, the recall and the F1 score.\n",
    " \n",
    "\n",
    "**Goals**\n",
    "In this part, you will learn how to:\n",
    "- Train a k-nn model\n",
    "- Evaluate the performance of the model\n",
    "- Find the best hyperparameters for the model\n",
    "- Print a learning curve\n",
    "- Compute performance metrics such as the confusion matrix, the accuracy, the precision, the recall and the F1 score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d32fa76",
   "metadata": {},
   "source": [
    "**NOTE: this section is dependent on the previous ones, so please make sure you run the previous section before running this one.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d54caaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the previous notebook, our data is stored in the following variables X_train_final, X_test_final, y_train, y_test\n",
    "# to avoid preprocessing the data again and again in the case we do something wrong (and we will), we store a copy of our variables in other variables\n",
    "# in case of problems, we can simply re-run this cell to get the pre-processed data back\n",
    "\n",
    "# NOTE: the names of the variables below should be the same of the variable at the end of Section 2\n",
    "\n",
    "X_tr = X_train_final.copy()\n",
    "X_te = X_test_final.copy()\n",
    "y_tr = y_train.copy().values.ravel()\n",
    "y_te = y_test.copy().values.ravel()\n",
    "\n",
    "\n",
    "# Explanation:\n",
    "# we use .values.ravel() to convert the pandas dataframe to a one-dimensional numpy array. This is necessary to avoid a warning message when training the model down below.\n",
    "# .values will give the values in a numpy array (shape: (n,1), this is actually a 2D array in which the second dimension has size 1)\n",
    "# .ravel() will convert that array shape to (n, ) (i.e. flatten it, so it is a 1D array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602cba0b",
   "metadata": {},
   "source": [
    "### 3.1 Train a k-nn model (naif way)\n",
    "\n",
    "We will use the Scikit-learn library to train a k-nn model. The k-nn algorithm is a non-parametric method used for classification and regression. In this algorithm, the input consists of the k closest training examples in the feature space. The output depends on whether k-nn is used for classification or regression:\n",
    "- In [k-nn classification](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small).\n",
    "- In [k-nn regression](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html), the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n",
    "\n",
    "In our case, we will use the k-nn algorithm for classification. In particular to predict if a passenger survived or not.\n",
    "\n",
    "In this first \"naif\" (bad) training, we simply train a model without any tuning. And we test the result in the test set.\n",
    "\n",
    "NOTE: this is a \"quick & dirty\" approach. We will see in the next sections how to properly train and evaluate a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46350c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the libraries\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adddf353",
   "metadata": {},
   "source": [
    "We will use the Scikit-learn library to train a k-nn model. The k-nn algorithm is a non-parametric method used for classification and regression. In this algorithm, the input consists of the k closest training examples in the feature space. The output depends on whether k-nn is used for classification or regression:\n",
    "- In [k-nn classification](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), the output is a class membership. An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small).\n",
    "- In [k-nn regression](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html), the output is the property value for the object. This value is the average of the values of its k nearest neighbors.\n",
    "\n",
    "In our case, we will use the k-nn algorithm for classification. In particular to predict if a passenger survived or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36040cd1",
   "metadata": {},
   "source": [
    "Let's train a k-nn model with the default parameters (k=5, euclidean distance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b60733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a k-nn model\n",
    "knn = KNeighborsClassifier(n_neighbors=5) # n_neighbors is the \"k\" parameter of k-nn\n",
    "knn.fit(X_tr, y_tr) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eac5829",
   "metadata": {},
   "source": [
    "After the call to the `.fit()` method, our knn is trained on the training set.\n",
    "\n",
    "We can now use the model to predict the labels of the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc9c9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels of the test set: y_pred\n",
    "y_pred = knn.predict(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22da443f",
   "metadata": {},
   "source": [
    "We can now evaluate the performance of the model by comparing the prediction to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc3ec61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "print(\"Test set predictions:\\n {}\".format(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807113fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the model\n",
    "print(\"Test set accuracy: {:.2f}\".format(knn.score(X_te, y_te)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df8aef",
   "metadata": {},
   "source": [
    "We should have an accuracy of ~0.80, which means that the model is correct 80% of the time on the test set.\n",
    "\n",
    "**Questions**:\n",
    "- Is considering only \"accuracy\" enough to evaluate the performance of a model? Why?\n",
    "- Are we sure that k=5 is the best value for the hyperparameter k? How can we find the best value for k?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162f879e",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "#TODO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6f6c77",
   "metadata": {},
   "source": [
    "**NOTE**: this was the \"naif\" way to train a model. In the next sections, we will see how to properly train and evaluate a model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158dd9e3",
   "metadata": {},
   "source": [
    "### 3.2 Train (and fine tune) the model using grid search and cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371134bc",
   "metadata": {},
   "source": [
    "In order to find the best `k`, we will use a \"*grid search*\" approach.\n",
    "\n",
    "Grid search is a method for parameter tuning that will methodically build and evaluate a model for each combination of algorithm parameters specified in a grid. It is called grid search because it searches across a grid of parameters.\n",
    "In our case we have just one hyperparameter: `k`. However, in general we can have more than one hyperparameter. In this case, we will have a \"grid\" of hyperparameters and Grid search will try *all* the combinations of hyperparameters.\n",
    "It is a good approach to find the best hyperparameters for a model but it can be computationally **very expensive** (we are retraining and testing a model for all possible combination of hyperparameters!).\n",
    "On the plus side, since each training is independent from the other, grid it can be parallelized easly. \n",
    "\n",
    "The function we will use is the `GridSearchCV` class. The \"CV\" at the end stands for \"Cross Validation\". Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. The function has a parameter called `cv` that defines the number of folds. It splits the training set into `cv` folds and uses `cv-1` folds for training and the remaining fold for testing. This process is repeated `cv` times, with each of the `cv` folds used exactly once as the validation data. The `GridSearchCV` class will use cross-validation to find the best hyperparameters for the model.\n",
    "\n",
    "Wrapping up, the `GridSearchCV` method will train the model for each combination of hyperparameters *cv* times!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e43c560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import the GridSearchCV class\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# we define the parameters that we want to test\n",
    "param_grid = {'n_neighbors': np.arange(1, 100)} # we test all \"k\" values from 1 to 100\n",
    "\n",
    "# we create a GridSearchCV object and fit it to the data (we use 5-fold cross-validation, and we use accuracy as the scoring metric)\n",
    "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5, scoring=\"accuracy\", return_train_score=True, verbose=1)\n",
    "best_knn = grid_search.fit(X_tr, y_tr) # we fit the model to the whole training set\n",
    "\n",
    "# we print the best parameters and the best score\n",
    "print(\"Best parameters: {}\".format(grid_search.best_params_))\n",
    "print(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ff88cf",
   "metadata": {},
   "source": [
    "**Question**:\n",
    "- What is the best `k`? What is the best accuracy score? What does it mean? Is this score related to training or test set?\n",
    "- Setting `verbose=1`, you should see something like \"Fitting 5 folds for each of 99 candidates, totalling 495 fits\". What does it mean?\n",
    "- Alternatives of `Grid Search` exist. One of these is `Random Search`. Explain the difference between Grid Search and Random Search. When should you use one over the other?\n",
    "- [Optional] Investigate the use of library such as [Optuna](https://optuna.org/) for hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e15c104",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e276ebde",
   "metadata": {},
   "source": [
    "#### 3.2.2 Plot the learning curve to see if the model is overfitting or underfitting\n",
    "\n",
    "The learning curve is a plot of the model's performance on the training set and the validation set as a function of the training set size. The purpose of a learning curve is to see how well the model is learning the data. In particular, we want to see if the model is overfitting or underfitting the data.\n",
    "\n",
    "In this section, we will plot the learning curve of two models: k-nn with k = 1 (knn_1_neighbor); k-nn with k = 23 (the best_knn we trained above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263132a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a k-nn model with k = 1\n",
    "knn_1_neighbor = KNeighborsClassifier(n_neighbors=1) # n_neighbors is the \"k\" parameter of k-nn\n",
    "knn_1_neighbor.fit(X_tr, y_tr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f992987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import LearningCurveDisplay, ShuffleSplit\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "common_params = {\n",
    "    \"X\": X_tr,\n",
    "    \"y\": y_tr,\n",
    "    \"train_sizes\": np.linspace(0.1, 1.0, 5), # 5 different sizes of the training set\n",
    "    \"cv\": ShuffleSplit(n_splits=50, test_size=0.2, random_state=0), # 50 repetitions of 80-20% cross-validation\n",
    "    \"score_type\": \"both\", # we want to see both training and validation scores\n",
    "    \"n_jobs\": -1,\n",
    "    \"line_kw\": {\"marker\": \"o\"},\n",
    "    \"std_display_style\": \"fill_between\",\n",
    "    \"score_name\": \"Accuracy\",\n",
    "}\n",
    "\n",
    "for ax_idx, estimator in enumerate([best_knn.best_estimator_, knn_1_neighbor]):\n",
    "    LearningCurveDisplay.from_estimator(estimator, **common_params, ax=ax[ax_idx])\n",
    "    handles, label = ax[ax_idx].get_legend_handles_labels()\n",
    "    ax[ax_idx].legend(handles[:2], [\"Training Score\", \"Validation Score\"])\n",
    "    ax[ax_idx].set_title(f\"Learning Curve for {estimator.__class__.__name__} with k={estimator.n_neighbors}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd75c396",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- What can you say about the learning curve of the model with k = 1? Is the model overfitting or underfitting the data?\n",
    "- What can you say about the learning curve of the model with k = 23? Is the model overfitting or underfitting the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981e652",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115aace",
   "metadata": {},
   "source": [
    "### Conclusion Section 3\n",
    "In this section, we learned how to use the Scikit-learn library to train and fine-tune a machine learning model. In particular, we learned how to use grid search and cross-validation to find the best hyperparameters for the model. We also learned how to plot the learning curve to see if the model is overfitting or underfitting the data.\n",
    "\n",
    "In the next section, you will perform the final evaluation of the model using on the **test set**. You will compute performance metrics such as the confusion matrix, the accuracy, the precision, the recall and the F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3e4d59",
   "metadata": {},
   "source": [
    "## 4 Evaluation\n",
    "\n",
    "We have the best possible model we can fit with the training set (at least in terms of *accuracy*). Now we can evaluate it on the test set. \n",
    "This should be the first time we use the test set. \n",
    "\n",
    "For the evaluation, we will:\n",
    "- make the predictions on the test set with the best modelwe have\n",
    "- plot the confusion matrix\n",
    "- compute the accuracy, the precision, the recall, and the F1 score\n",
    "- plot the learning curve (to evaluate possible overfitting or underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499f4c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions\n",
    "y_pred = best_knn.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c199e78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix (simple)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion = confusion_matrix(y_te, y_pred)\n",
    "print('Confusion matrix:\\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b4ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix (fancy)\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# we create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix (fancy and normalized)\n",
    "confusion_normalized = confusion / confusion.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "# we create a heatmap of the normalized confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(confusion_normalized, annot=True, cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Normalized Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18af48bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print('Classification report:\\n', classification_report(y_te, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3eb489a",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- Are you happy with the result? Why?\n",
    "- The results that you got on the training set are very different from the results on the test set. What does it mean?\n",
    "- What could you do to improve the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f17479",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e492882b",
   "metadata": {},
   "source": [
    "### Conclusion Section 4\n",
    "In this section, we evaluated the performance of a k-nn model on the test set. We computed metrics such as the confusion matrix, the accuracy, the precision, the recall and the F1 score.\n",
    "\n",
    "This completes a full Machine Learning workflow. We started from the data exploration, then we prepared the data for the training, we trained and fine-tuned a model, and finally we evaluated the model on the test set. In the next and last section, you will modify the code above to train and evaluate a SVM model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa5351d",
   "metadata": {},
   "source": [
    "## 5 Support Vector Machine (SVM)\n",
    "\n",
    "In this section you will train and evaluate a Support Vector Machine (SVM) model. We suggest that you directly modify the code above to train, fine-tune, and evaluate a SVM model in addition to the k-nn model. In the evaluation section, you can compare the performance of the two models.\n",
    "\n",
    "We suggest using the [SVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) class from the Scikit-learn library to train a SVM model. The `SVC` class implements the Support Vector Classification algorithm.\n",
    "\n",
    "Once implemented and tested, answer the following question below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5467561e",
   "metadata": {},
   "source": [
    "**Questions**:\n",
    "- Which model performs better? (k-nn or SVM)? Why do you think so?\n",
    "- Which hyperparameters did you tune for the SVM model? What's the best combination of hyperparameters you found?\n",
    "- Which model is faster to train? (k-nn or SVM)? Why do you think so?\n",
    "- Discuss the advantages and disadvantages of the two models (k-nn and SVM). Investigate the idea of Lazy learning vs Eager learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d7aed",
   "metadata": {},
   "source": [
    "**Answers**:\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dbb778",
   "metadata": {},
   "source": [
    "### Conclusion Section 5\n",
    "\n",
    "In this section, you learned how to use the Scikit-learn library to train and evaluate a Support Vector Machine (SVM) model. You also compared the performance of the SVM model with the k-nn model. Now you should have a quite good understanding of the Machine Learning workflow using the Scikit-learn library, in particular for classification tasks.\n",
    "\n",
    "#### [Optional] Going further - Regression task\n",
    "In this project, we performed a classification task. However, k-nn and SVM can also be used to perform regression tasks.\n",
    "We could change the project to a regression task by defining a new target (e.g., the `Age` of the passenger).\n",
    "\n",
    "Most of the steps are the same, but there are some important differences. For example, in a regression task:\n",
    "- You need to use other metrics such as the mean squared error (MSE) or the mean absolute error (MAE) score.\n",
    "- You need to use other classes such as the `KNeighborsRegressor` or the `SVR` class instead of the `KNeighborsClassifier` or the `SVC` class."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
